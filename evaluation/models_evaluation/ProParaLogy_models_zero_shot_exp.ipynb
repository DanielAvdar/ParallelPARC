{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to drive"
      ],
      "metadata": {
        "id": "txYbix1fzHqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',  force_remount=True)\n",
        "proparalogy_folder_path = 'gdrive/MyDrive/proparalogy'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXkejodazNDl",
        "outputId": "593d1f4b-9d02-4245-e55d-1fa62c416df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU details"
      ],
      "metadata": {
        "id": "IBFwtcWAzQc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "h19GUteICyaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23TOba33L4qf",
        "outputId": "4e3193f6-33ab-4aae-e5c4-0239b87c54c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jul 25 20:46:23 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "# Clear GPU memory\n",
        "def clear_gpu_memory():\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()"
      ],
      "metadata": {
        "id": "k6TNLBO9iagW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTGF06zNNFNK",
        "outputId": "fe8a03af-ef0a-44f1-b982-0031aa74fed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Libs"
      ],
      "metadata": {
        "id": "7SND19ehzj-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --quiet\n",
        "!pip install torch --quiet\n",
        "!pip install openai --quiet\n",
        "!pip install accelerate --quiet"
      ],
      "metadata": {
        "id": "4DjiRsUUzmbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b49851eb-5606-40c4-a6a4-3e1cb5642ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "ObEl2_sOz_C3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "import json\n",
        "\n",
        "def create_folder(path):\n",
        "  if not os.path.exists(path):\n",
        "      os.makedirs(path)\n",
        "  return path\n",
        "\n",
        "def create_json(path):\n",
        "  if not os.path.isfile(path):\n",
        "    print('Creating json ', path)\n",
        "    with io.open(path, 'w+') as json_file:\n",
        "      json_file.write(json.dumps({}))\n",
        "  return path\n",
        "\n",
        "PROJECT_PATH = proparalogy_folder_path\n",
        "ASSETS_DISTRACTORS = create_folder(PROJECT_PATH + '/distractors/')\n",
        "PROMPTS_PATH = create_folder(PROJECT_PATH + '/prompts/')\n",
        "MODEL_RESULTS_PATH = create_folder(PROJECT_PATH + '/models_results')\n",
        "EXPERIMENTS_PATH = create_folder(PROJECT_PATH + '/experiments')\n",
        "EXPERIMENTS_RESULTS_PATH = create_folder(PROJECT_PATH + '/experiments_results')\n",
        "\n",
        "columns_to_serialize = ['candidates']\n",
        "\n",
        "GPT_CACHE_PATH = create_json(PROJECT_PATH + '/gpt_response_cache.json')\n",
        "FLAN_T5_CACHE_PATH = create_json(PROJECT_PATH + '/flan_t5_prediction_cache.json')\n",
        "OPEN_AI_KEY = open(PROJECT_PATH + '/OPEN_AI_KEY.txt', 'r').read()\n",
        "\n",
        "MULTIPLE_CHOICE_PROMPT_PARAGRAPHS = open(PROJECT_PATH + '/prompts/multiple_choice_prompt_dafna.txt', 'r').read()\n",
        "BINARY_PROMPT_PARAGRAPHS = open(PROJECT_PATH + '/prompts/binary_prompt_dafna.txt', 'r').read()\n",
        "\n",
        "MODELS = {\n",
        "    'ChatGPT': 'gpt-3.5-turbo',\n",
        "    'GPT4': 'gpt-4',\n",
        "    'FLAN-T5-small': 'google/flan-t5-small',\n",
        "    'FLAN-T5-xl': 'google/flan-t5-xl',\n",
        "    'FLAN-T5-xxl': 'google/flan-t5-xxl'\n",
        "}"
      ],
      "metadata": {
        "id": "UqJJwcxX0CK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BINARY_PROMPT_PARAGRAPHS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "3dmdE_DSSye3",
        "outputId": "fb74cacf-31af-4dd3-8aa7-67a1e0a4d61e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In this task, you\\'ll be given two paragraphs that describe scientific processes. Your goal is to decide whether the processes are analogous.\\nAnalogy is a mapping in which the objects of one process are structurally aligned with the objects of another.\\nIt is based on similarity of the relationships between the objects and the roles they play throughout the process, and not on the similarity between object attributes.\\nFor example, there is an analogy between a paragraph about \"How does an electrical circuit work?\", and a paragraph about \"How does a mechanical water pump work?\". In this analogy, electrons are mapped to water: both start at some state (low voltage/low pressure), then move through something (wire/pipe), and change their state (high voltage/high pressure) because of another object (battery/pump).\\nSimilar first order relations between the domains include:\\n(battery, creates, electrical voltage) like (pump, generates, pressure)\\n(electrons, move through, copper wire) like (water, flows through, pipe).\\nOn the other hand, if for example the second paragraph about the pump is describing that: first the water flows inside the pipe, and following this the pump creates pressure, it changes the cause and effect relationship (higher order relation) to be different from the first paragraph about the electrical circuit, and in this case, the processes are not analogous.\\nAnswer \"1\" if the two paragraphs describe analogous processes, and \"0\" if not.\\n\\nFirst Paragraph:\\n{}\\nSecond Paragraph:\\n{}\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "6ebBvHF1-fnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "MULTIPLE_CHOICE_TASK_TYPE = 'MULTIPLE_CHOICE'\n",
        "BINARY_TASK_TYPE = 'BINARY'\n",
        "\n",
        "def convert(o):\n",
        "    if isinstance(o, np.int64) or isinstance(o, np.int32):\n",
        "        return int(o)\n",
        "    raise TypeError\n",
        "\n",
        "\n",
        "def dump_json(file_path, data, indent=4, sort_keys=True):\n",
        "    with open(file_path, 'w') as f:\n",
        "        string_json = json.dumps(data)\n",
        "        f.write(string_json)\n",
        "        f.close()\n",
        "\n",
        "def get_json(file_path, create_if_not_exist=False):\n",
        "    with open(os.path.join(file_path), encoding=\"utf8\") as f:\n",
        "        response = json.load(f)\n",
        "        f.close()\n",
        "        return response\n",
        "\n",
        "def json_dumps_df(df, columns_to_serialize):\n",
        "    for c in columns_to_serialize:\n",
        "        if c in df:\n",
        "            df[c] = df[c].apply(json.dumps)\n",
        "    return df\n",
        "\n",
        "def extract_multuiple_choice_candidate_from_model_response(response):\n",
        "  try:\n",
        "    response = response.strip().lower()\n",
        "    response_candidates = re.findall(r'\\bc\\d+\\b', response)\n",
        "    assert len(response_candidates) == 1\n",
        "    return response_candidates[0].upper()\n",
        "  except:\n",
        "    print(response)\n",
        "    return None\n",
        "\n",
        "def get_multiple_choice_task_prompt(r, candidates, is_one_shot, is_few_shot, is_relations):\n",
        "    return MULTIPLE_CHOICE_PROMPT_PARAGRAPHS.format(r['source_paragraph'], *candidates)\n",
        "\n",
        "def get_binary_task_prompt(r, candidates, is_one_shot, is_few_shot, is_relations):\n",
        "    return BINARY_PROMPT_PARAGRAPHS.format(r['source_paragraph'], *candidates)\n",
        "\n",
        "def get_task_type(task_name):\n",
        "  if 'multiple_choice_task' in task_name.lower():\n",
        "    return MULTIPLE_CHOICE_TASK_TYPE\n",
        "  if 'binary_task' in task_name.lower():\n",
        "    return BINARY_TASK_TYPE\n",
        "  else:\n",
        "    raise Exception('Task name should contain either \\\"multiple_choice_task\\\" or \\\"binary_task\\\" corresponding to the task type.')"
      ],
      "metadata": {
        "id": "PjrbGHsU-e4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task Evaluators"
      ],
      "metadata": {
        "id": "fAVhHPuPD7Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "from abc import (\n",
        "    ABC,\n",
        "    abstractmethod,\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "_TASK_MULTIPLE_CHOICE = 'multiple_choice_classification'\n",
        "_TASK_BINARY_CLASSIFICATION = 'binary_classification'\n",
        "\n",
        "class TaskEvaluator(ABC):\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.num_of_correct_answers = None\n",
        "\n",
        "    def get_eval_metrics(self):\n",
        "        return {'accuracy': accuracy_score, 'f1 score': f1_score}\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_ground_truth(self, r):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_predictions(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_prompt(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def parse_model_prediction(self):\n",
        "        pass\n",
        "\n",
        "    def process_data(self, data):\n",
        "        return data\n",
        "\n",
        "    def get_ground_truth(self, r):\n",
        "        return r['ground_truth']\n",
        "\n",
        "    def get_scores(self, models_predictions_data):\n",
        "        all_predictions = []\n",
        "        mapped_scores = {}\n",
        "        for model, model_predictions in models_predictions_data.items():\n",
        "          ground_truths = list(map(lambda model_prediction: model_prediction['ground_truth'], model_predictions))\n",
        "          model_predictions = list(map(lambda model_prediction: model_prediction[model], model_predictions))\n",
        "          all_predictions.extend(model_predictions)\n",
        "\n",
        "          for metric_name, metric in self.get_eval_metrics().items():\n",
        "            eval_metric_score = metric(ground_truths, model_predictions)\n",
        "            mapped_scores[f'metric_{metric_name}_score_{model}'] = round(eval_metric_score * 100, 2)\n",
        "        return pd.Series(mapped_scores), pd.DataFrame(all_predictions)\n",
        "\n",
        "\n",
        "    def get_row_candidates(self, r, index):\n",
        "        return json.loads(r['shuffled_candidates'])\n",
        "\n",
        "\n",
        "class TaskMultipleChoiceClassification(TaskEvaluator):\n",
        "    _CLASS = 'class'\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.num_of_correct_answers = 1\n",
        "\n",
        "    def get_predictions(self, model, r, candidates, is_one_shot, is_few_shot, is_relations):\n",
        "        prompt = self.get_prompt(r, candidates, is_one_shot, is_few_shot, is_relations)\n",
        "        model_response = model.get_model_prediction(prompt)\n",
        "        model_answer = extract_multuiple_choice_candidate_from_model_response(model_response)\n",
        "        answer_candidate, answer_candidate_index =  self.parse_model_prediction(model_answer, candidates, r[\"sample_id\"], model)\n",
        "        return answer_candidate, answer_candidate_index, prompt, model_response\n",
        "\n",
        "    def get_prompt(self, r, candidates, is_one_shot, is_few_shot, is_relations):\n",
        "        return get_multiple_choice_task_prompt(r, candidates, is_one_shot, is_few_shot, is_relations)\n",
        "\n",
        "    def parse_model_prediction(self, answer, candidates, instance_id, model):\n",
        "        if answer is not None:\n",
        "            candidate_index = int(answer[1:])\n",
        "            assert answer.startswith('C') and 1 <= candidate_index <= len(candidates)  # valid candidate of C{int}\n",
        "            return candidates[candidate_index - 1], answer  # array index for C1 is 0\n",
        "        else:\n",
        "            print(f'Model {model} failed to predict answer on {instance_id} choosing random candidate.')\n",
        "            answer_candidate = random.choice(candidates)\n",
        "            return answer_candidate, f'C{str(candidates.index(answer_candidate))}'\n",
        "\n",
        "    def get_eval_metrics(self):\n",
        "        return {'accuracy': accuracy_score}\n",
        "\n",
        "class TaskBinaryClassification(TaskEvaluator):\n",
        "    _CLASS = 'class'\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.num_of_correct_answers = 1\n",
        "\n",
        "    def get_prompt(self, r, candidates, is_one_shot, is_few_shot, is_relations):\n",
        "        return get_binary_task_prompt(r, candidates, is_one_shot, is_few_shot, is_relations)\n",
        "\n",
        "    def parse_model_prediction(self, model, model_response, sample_id):\n",
        "      try:\n",
        "        model_answer = int(model_response.replace('<pad>', '').replace('</s>', ''))\n",
        "        assert model_answer in [0, 1]\n",
        "        return model_answer\n",
        "      except:\n",
        "        print(f'Model {model} failed to return answer in requsted format (sample id: {sample_id}). Instead it retruned \\\"{model_response}\\\"')\n",
        "        return random.randint(0,1)\n",
        "\n",
        "    def get_predictions(self, model, r, candidates, is_one_shot, is_few_shot, is_relations):\n",
        "        prompt = self.get_prompt(r, candidates, is_one_shot, is_few_shot, is_relations)\n",
        "        model_response = model.get_model_prediction(prompt)\n",
        "        answer_candidate =  self.parse_model_prediction(model, model_response, r[\"sample_id\"])\n",
        "        return answer_candidate, answer_candidate, prompt, model_response\n",
        "\n",
        "    def get_row_candidates(self, r, index):\n",
        "        return [r['target_paragraph']]\n",
        "\n",
        "\n",
        "task_map = {_TASK_MULTIPLE_CHOICE: TaskMultipleChoiceClassification, _TASK_BINARY_CLASSIFICATION: TaskBinaryClassification}\n",
        "\n",
        "def get_task_evaluator(task_type):\n",
        "  if task_type == MULTIPLE_CHOICE_TASK_TYPE:\n",
        "    return task_map[_TASK_MULTIPLE_CHOICE]\n",
        "  if task_type == BINARY_TASK_TYPE:\n",
        "    return task_map[_TASK_BINARY_CLASSIFICATION]\n",
        "  else:\n",
        "    raise Exception('No TaskEvaloator found, invalid task type.')"
      ],
      "metadata": {
        "id": "2BdGy1BD_cUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT utils"
      ],
      "metadata": {
        "id": "HRSME5tUzefe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = OPEN_AI_KEY #OPEN_AI_KEY\n",
        "GPT_CACHE = get_json(GPT_CACHE_PATH)\n",
        "\n",
        "def init_gpt_cache():\n",
        "    global GPT_CACHE\n",
        "    GPT_CACHE = get_json(GPT_CACHE_PATH)\n",
        "\n",
        "def get_chatgpt_3_5_multiple_choice_prediction(prompt):\n",
        "    try: # in case model is overloaded\n",
        "        return get_chat_gpt_prediction(prompt, MODELS['ChatGPT'])\n",
        "    except:\n",
        "        return get_chat_gpt_prediction(prompt, MODELS['ChatGPT'])\n",
        "\n",
        "def get_gpt_4_multiple_choice_prediction(prompt):\n",
        "    return get_chat_gpt_prediction(prompt, MODELS['GPT4'])\n",
        "\n",
        "def get_chat_gpt_prediction(prompt, model):\n",
        "    cached_response = get_gpt_response_from_cache(prompt, model)\n",
        "\n",
        "    if cached_response is not None:\n",
        "        response = cached_response.get('choices')[0].get('message').get('content')\n",
        "        return response\n",
        "\n",
        "    output = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    save_gpt_response_in_cache(prompt, model, output)\n",
        "    response = output.get('choices')[0].get('message').get('content')\n",
        "    return response\n",
        "\n",
        "def get_gpt_cache_id(prompt, model):\n",
        "    return f\"{prompt}___{model}___{1}\".replace('\\r', '')\n",
        "\n",
        "def get_gpt_response_from_cache(prompt, model):\n",
        "    id = get_gpt_cache_id(prompt, model)\n",
        "    return GPT_CACHE.get(id)\n",
        "\n",
        "def save_gpt_response_in_cache(prompt, model, response):\n",
        "    GPT_CACHE.update({get_gpt_cache_id(prompt, model): response})\n",
        "    dump_json(GPT_CACHE_PATH, GPT_CACHE)\n"
      ],
      "metadata": {
        "id": "5ZJKCEr6zh2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FLAN T5 Utils"
      ],
      "metadata": {
        "id": "1B3zmSI3NXOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FLAN_CACHE = get_json(FLAN_T5_CACHE_PATH)\n",
        "# This is the defualt value of model.generate() function\n",
        "# Provided due to deprication warning\n",
        "MAX_TOKENS=20\n",
        "\n",
        "def init_flan_cache():\n",
        "    global FLAN_CACHE\n",
        "    FLAN_CACHE = get_json(FLAN_T5_CACHE_PATH)\n",
        "\n",
        "def get_flan_t5_multiple_choice_predictions(prompt, model, tokenizer, model_name):\n",
        "    cached_response = get_flan_t5_response_from_cache(prompt, model_name)\n",
        "\n",
        "    if cached_response is not None:\n",
        "        return cached_response\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "        outputs = model.generate(input_ids, max_length=MAX_TOKENS)\n",
        "    model_prediction = tokenizer.decode(outputs[0])\n",
        "    save_flan_t5_response_in_cache(prompt, model_name, model_prediction)\n",
        "    return model_prediction\n",
        "\n",
        "def get_flan_t5_cache_id(prompt, model):\n",
        "    return f\"{prompt}___{model}___{MAX_TOKENS}\"\n",
        "\n",
        "def get_flan_t5_response_from_cache(prompt, model):\n",
        "    id = get_flan_t5_cache_id(prompt, model).replace('\\r', '')\n",
        "    return FLAN_CACHE.get(id)\n",
        "\n",
        "def save_flan_t5_response_in_cache(prompt, model, response):\n",
        "    FLAN_CACHE.update({get_flan_t5_cache_id(prompt, model): response})\n",
        "    dump_json(FLAN_T5_CACHE_PATH, FLAN_CACHE)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4nnGSPlYNWcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-Shot Model Class"
      ],
      "metadata": {
        "id": "A2Im2WrVNwxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "\n",
        "class ZeroShotModel:\n",
        "    def __init__(self, model_description):\n",
        "        self.model_description = model_description\n",
        "        if 'T5' in self.model_description:\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(MODELS[self.model_description], torch_dtype=torch.float16)\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(MODELS[self.model_description])\n",
        "            self.model.to(device)\n",
        "\n",
        "    def get_model_prediction(self, prompt):\n",
        "        if self.model_description == 'ChatGPT':\n",
        "            response = get_chatgpt_3_5_multiple_choice_prediction(prompt)\n",
        "        elif self.model_description == 'GPT4':\n",
        "            response = get_gpt_4_multiple_choice_prediction(prompt)\n",
        "        elif 'T5' in self.model_description:\n",
        "            response = get_flan_t5_multiple_choice_predictions(prompt, self.model, self.tokenizer, self.model_description)\n",
        "        else:\n",
        "            raise Exception()\n",
        "        return response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CC_2Pm8NwJW",
        "outputId": "40e2e380-0187-496e-ab4a-ec97b86d046c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Zero-shot"
      ],
      "metadata": {
        "id": "uSLwoYs0dcfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "init_gpt_cache()\n",
        "init_flan_cache()\n",
        "\n",
        "def main(args):\n",
        "    print(f\"TASK: {args.task}\")\n",
        "\n",
        "    # This is for comparing the print - remove on prod\n",
        "    print_list = {'ground_truth': []}\n",
        "    models_predictions_data = {}\n",
        "    for model_name, model in models_list.items():\n",
        "      print_list.update({model_name: []})\n",
        "      models_predictions_data.update({model_name: []})\n",
        "    # ------------------------------------------------\n",
        "\n",
        "    task_evaluator = get_task_evaluator(args.task_type)(args.task)\n",
        "    df = pd.read_csv(f'{EXPERIMENTS_PATH}/{args.task}.csv', encoding='utf8')\n",
        "    # args.task = task + ('_with_paragraphs' if IS_PROMPT_WITH_PARAGRAPHS else '_no_paragraphs')\n",
        "    print(f\"Read {len(df)}\")\n",
        "    df = task_evaluator.process_data(df)\n",
        "    predictions_metadata = []\n",
        "    for idx, (r_idx, r) in enumerate(tqdm(df.iterrows(), desc=f'Solving Dataset ({args.task})', total=len(df))):\n",
        "        candidates = task_evaluator.get_row_candidates(r, idx)\n",
        "        ground_truth = task_evaluator.get_ground_truth(r)\n",
        "        # Remove in prod\n",
        "        print_list.get('ground_truth').append(ground_truth)\n",
        "        # --------------\n",
        "        for model_name, model in models_list.items():\n",
        "            prediction, predicted_labels, prompt, response = task_evaluator.get_predictions(model, r, candidates, args.one_shot, args.few_shot, args.relations)\n",
        "            predictions_metadata.append(get_prediction_data_dictionary(r, model_name, prediction, predicted_labels, ground_truth, prompt, response))\n",
        "            models_predictions_data.get(model_name).append({model.model_description: predicted_labels, 'ground_truth': ground_truth})\n",
        "\n",
        "            # Remove in prod\n",
        "            print_list.get(model_name).append(predicted_labels)\n",
        "            # --------------\n",
        "\n",
        "\n",
        "        if args.debug and idx == 99:\n",
        "            print(f\"Debug, exiting\")\n",
        "            break\n",
        "\n",
        "    export_results(task_evaluator, args, models_predictions_data)\n",
        "\n",
        "    predictions_metadata_path = os.path.join(EXPERIMENTS_RESULTS_PATH, f\"{args.task}_predictions_metadata.csv\")\n",
        "    predictions_metadata = pd.DataFrame(predictions_metadata)\n",
        "    predictions_metadata.to_csv(predictions_metadata_path, index=True, encoding='utf-8')\n",
        "\n",
        "    ####### print list - remove in prod #########\n",
        "    for model_name in print_list.keys():\n",
        "      if model_name == 'ground_truth':\n",
        "        continue\n",
        "      print(f'\\n\\nModel {model_name}')\n",
        "      print(print_list.get(model_name))\n",
        "      print(print_list.get('ground_truth'))\n",
        "      print()\n",
        "    # -------------------------------------------\n",
        "    print(\"Done\")\n",
        "\n",
        "\n",
        "def get_prediction_data_dictionary(r, model_name, prediction, predicted_labels, ground_truth, prompt, response):\n",
        "   csv_task_row = json.loads(r.to_json())\n",
        "   return {**csv_task_row, 'model': model_name, 'prediction': prediction, 'prediction_index': predicted_labels, 'ground_truth': ground_truth\n",
        "           , 'prompt': prompt, 'response': response}\n",
        "\n",
        "\n",
        "def export_results(task_evaluator, args, models_predictions_data):\n",
        "    scores, predictions = task_evaluator.get_scores(models_predictions_data)\n",
        "    predictions = json_dumps_df(predictions, columns_to_serialize)\n",
        "    print(f\"\\n---- Results - {args.task} ----\\n\")\n",
        "    print(scores)\n",
        "    out_p = os.path.join(EXPERIMENTS_RESULTS_PATH, f\"predictions_{args.task}.csv\")\n",
        "    print(f\"\\nWriting predictions to {out_p}\")\n",
        "    predictions.to_csv(out_p, index=False, encoding='utf-8')\n",
        "    out_p_mean = os.path.join(EXPERIMENTS_RESULTS_PATH, f\"score_results_{args.task}.csv\")\n",
        "    print(f\"Writing score results to {out_p_mean}\")\n",
        "    scores.to_csv(out_p_mean, index=True, encoding='utf-8')\n",
        "\n",
        "\n",
        "def initialize_models(args):\n",
        "    global models_list\n",
        "    try:\n",
        "      if models_list is None:\n",
        "        models_list = {}\n",
        "    except:\n",
        "      models_list = {}\n",
        "    for model in args.models_to_run:\n",
        "      if models_list.get(model) is None:\n",
        "        zeroshot_model = ZeroShotModel(model)\n",
        "        models_list[model] = zeroshot_model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    for task in [\n",
        "      \"data_for_eval_balanced_shuffled_binary_task\"\n",
        "      \"data_for_eval_random_candidates_multiple_choice_task\",\n",
        "    ]:\n",
        "        parser = argparse.ArgumentParser()\n",
        "        print(f'Task: {task}')\n",
        "        parser.add_argument('--models_to_run', nargs=\"+\", default=list(MODELS.keys()))\n",
        "        parser.add_argument('--task', default=task)\n",
        "        parser.add_argument('--task_type', default=get_task_type(task))\n",
        "        parser.add_argument('--one_shot', default=False)\n",
        "        parser.add_argument('--few_shot', default=False)\n",
        "        parser.add_argument('--relations', default=False)\n",
        "        parser.add_argument(\"--debug\", action='store_const', default=False, const=True)\n",
        "        args = parser.parse_args(args=[])\n",
        "\n",
        "        print(args)\n",
        "        initialize_models(args)\n",
        "\n",
        "        main(args)\n"
      ],
      "metadata": {
        "id": "2d5UeFghdcfM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}